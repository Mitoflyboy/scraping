{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rental Accomodation Booking Predictor\n",
    "\n",
    "For the last 3 months I have been running some web scraping jobs to extract the publicly available data for short term holiday rentals in NSW from an AirBnB type of website. Extracting the property details as well as the public calendar allows us to get both the property features as well as see the current state of bookings, specifically which dates are currently booked and for how long.\n",
    "\n",
    "The image below shows the location of all the properties, with Sydney shown as the red dot:\n",
    "\n",
    "![Property Locations](Airbnb_Property_Locations_NSW.PNG)\n",
    "\n",
    "\n",
    "Features that we are able to extract include:\n",
    "* Number of bedrooms\n",
    "* Number of guests allowed\n",
    "* Price per night\n",
    "* Overall rating and the number of reviews given\n",
    "* Distance and direction from Sydney\n",
    "* Number of photos of the property on the listing\n",
    "* Word count of the main description\n",
    "* The type of property (cottage, apartment, farmstay, house etc)\n",
    "* Other features extracted from the listing begin with 'f_' such as ```f_clothes_dryer```\n",
    "* Calendar showing current forward bookings\n",
    "\n",
    "The calendar information shows arrival and departure dates as part of the html code, which can be parsed with Python and then collated to provide the number of days booked. Future improvements will include breaking this down into the number of weekends, public holidays, and school holiday bookings.\n",
    "\n",
    "![Calendar](Airbnb_Calendar.PNG)\n",
    "\n",
    "From manual inspection there are a small number of property listings who get high bookings, with a large number of property listings having very few or zero bookings.\n",
    "\n",
    "The hypothesis is that there is an identifiable feature list which is desirable for bookings, and therefore that bookings can be predicted accurately for a new property when given the full set of features of that property.\n",
    "\n",
    "\n",
    "\n",
    "Based on the tutorial at https://mitrai.com/tech-guide/using-aws-sagemaker-linear-regression-to-predict-store-transactions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'deloitte-sagemaker'\n",
    "prefix = 'sagemaker/linear_time_series_forecast'\n",
    "\n",
    "# Define IAM role\n",
    "#role = get_execution_role()\n",
    "#role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Import and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_1 = pd.read_csv('housing_sample.csv')\n",
    "\n",
    "# Fill any 'NaN' values with zero\n",
    "hd_1.fillna(0,inplace=True)\n",
    "\n",
    "# Drop the 'ext_at' column\n",
    "hd_1.drop('ext_at',axis=1, inplace=True)\n",
    "\n",
    "# Exclude any rows there the 'init_price' is unknown.\n",
    "hd_2 = hd_1[hd_1['init_price'] > 0 ]\n",
    "\n",
    "# Drop the 'property_id' column\n",
    "hd_3 = hd_2.drop('property_id',axis=1)\n",
    "\n",
    "h_data = hd_3\n",
    "\n",
    "\n",
    "display(h_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing postcodes\n",
    "mp = h_data[h_data['postcode'].isnull()]\n",
    "mp.postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that all fields are non null\n",
    "h_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Test Data Sets\n",
    "\n",
    "We will use the ```total_booked_days``` column as being the y-value we want to predict, then create training and test data sets using an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which column is the y variable\n",
    "y = h_data.total_booked_days\n",
    "\n",
    "# Split into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(h_data, y, test_size=0.20, random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Data and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training dataset to RecordIO format as required by Amazon Sagemaker\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, np.array(X_train).astype('float32'), np.array(y_train).astype('float32'))\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data to S3\n",
    "\n",
    "key = 'housing_data.data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert validation dataset to RecordIO format as required by Amazon Sagemaker\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, np.array(X_test).astype('float32'), np.array(y_test).astype('float32'))\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload validation data to S3\n",
    "\n",
    "key = 'housing_validation.data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', key)).upload_fileobj(buf)\n",
    "s3_validation_data = 's3://{}/{}/validation/{}'.format(bucket, prefix, key)\n",
    "print('Uploaded validation data location: {}'.format(s3_validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run LinearLearner Predictor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the containers\n",
    "\n",
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest',\n",
    "             'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest',\n",
    "             'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest',\n",
    "             'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Sagemaker session\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(containers[boto3.Session().region_name]\n",
    "                                       ,role\n",
    "                                       ,train_instance_count=1\n",
    "                                       ,train_instance_type='ml.m5.large'\n",
    "                                       ,output_path='s3://{}/{}/output'.format(bucket, prefix)\n",
    "                                       ,sagemaker_session=sess)\n",
    "\n",
    "linear.set_hyperparameters(feature_dim=77\n",
    "                           ,mini_batch_size=100\n",
    "                           ,predictor_type='regressor'\n",
    "                           ,epochs=10\n",
    "                           ,num_models=32\n",
    "                           ,loss='absolute_loss')\n",
    "\n",
    "linear.fit({'train': s3_train_data, 'validation': s3_validation_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor = linear.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to convert our numpy arrays into a format that can be handled by the HTTP POST request we pass to the inference container. In this case it’s a simple CSV string. The results will be published back as JSON. For these common formats we can use the Amazon SageMaker Python SDK’s built in csv_serializer and json_deserializer functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Model with Test Dataset\n",
    "\n",
    "Now that the model has been deployed to an endpoint we can call this model with the test dataset, then extract the predictions into the ```one_step``` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = X_test.as_matrix()\n",
    "\n",
    "result = linear_predictor.predict(test_X)\n",
    "\n",
    "one_step = np.array([r['score'] for r in result['predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find some values from the test data to validate where the number of booked days is non zero\n",
    "\n",
    "X_test['total_booked_days'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the first item (index position 0) has some bookings we can look at the result vs the predicted result\n",
    "test_index = 0\n",
    "\n",
    "pred_price = one_step[test_index]\n",
    "print(\"Prediction: \" + str(pred_price))\n",
    "\n",
    "act_price = X_test.iloc[test_index]['total_booked_days']\n",
    "print(\"Actual: \" + str(act_price))\n",
    "\n",
    "print(\"Difference: {0:.2f}%\".format((((act_price - pred_price)/act_price)*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work out the difference between the prediction and the results for all the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "\n",
    "res1 = (np.abs(y_test - one_step) / y_test)\n",
    "\n",
    "# If the result is 'inf' then default to zero\n",
    "res1[res1 == inf] = 0\n",
    "\n",
    "print(\"Median differencet: \" + str(np.median(res1)))\n",
    "\n",
    "# Check the first few records\n",
    "res1[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Graph Results\n",
    "Graph the actual values vs predicted values for a slice of the dataset.\n",
    "\n",
    "**Note:** The predicted values are all within a fraction of the actual result, ie 17.0 vs 16.818 for the first element. This means that the results are hard to see visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('One-step = ', np.median(res1))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(one_step[200:250], label='forecast')\n",
    "plt.plot(np.array(y_test[200:250]), label='actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some housing details manually to see what changes\n",
    "\n",
    "#linear_predictor.predict() #expected label to be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you're ready to be done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.delete_endpoint(linear_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
