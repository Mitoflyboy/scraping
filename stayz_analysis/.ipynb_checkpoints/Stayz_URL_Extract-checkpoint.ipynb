{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def get_base_urls(by_area=False):\n",
    "\n",
    "    # Use the previous days extract to run todays suburbs\n",
    "    #date_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    #date_str ='2018-03-22'\n",
    "\n",
    "    # Read the data file and display\n",
    "    nsw_data = pd.read_json('/Users/taj/GitHub/scraping/stayz/WebData/nsw_extract/stayz_nsw_extract_' + date_str + '.json')\n",
    "\n",
    "    nsw_urls = nsw_data['url']\n",
    "\n",
    "    # List to keep all the base urls\n",
    "    base_urls = []\n",
    "\n",
    "    # If we want the breakdown by individual area then call with by_area = True\n",
    "    if by_area:\n",
    "\n",
    "        print(\"Collecting by individual area...\")\n",
    "\n",
    "        for u in nsw_urls:\n",
    "\n",
    "            u_s = u.split('/')\n",
    "\n",
    "            suburb = u_s[-2]\n",
    "            area = u_s[-3]\n",
    "\n",
    "            url = 'https://www.stayz.com.au/accommodation/nsw/' + area + '/' + suburb\n",
    "\n",
    "            if url not in base_urls:\n",
    "                base_urls.append(url)\n",
    "            #else:\n",
    "            #    print(\"Duplicate\")\n",
    "    else:\n",
    "        print(\"Collecing for whole state of NSW\")\n",
    "        base_urls.append('https://www.stayz.com.au/accommodation/nsw')\n",
    "\n",
    "\n",
    "    # Find why the others were not identified?? 4k missing??\n",
    "    return base_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nsw_urls(by_area=False):\n",
    "\n",
    "    # Use the previous days extract to run todays suburbs\n",
    "    #date_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    #date_str ='2018-03-22'\n",
    "\n",
    "    # Read the data file and display\n",
    "    nsw_data = pd.read_json('/Users/taj/GitHub/scraping/stayz/WebData/nsw_extract/stayz_nsw_extract_' + date_str + '.json')\n",
    "\n",
    "    nsw_urls = nsw_data['url']\n",
    "\n",
    "    # List to keep all the base urls\n",
    "    base_urls = []\n",
    "\n",
    "    # If we want the breakdown by individual area then call with by_area = True\n",
    "    if by_area:\n",
    "\n",
    "        print(\"Collecting by individual area...\")\n",
    "\n",
    "        for u in nsw_urls:\n",
    "\n",
    "            u_s = u.split('/')\n",
    "\n",
    "            suburb = u_s[-2]\n",
    "            area = u_s[-3]\n",
    "\n",
    "            url = 'https://www.stayz.com.au/accommodation/nsw/' + area + '/' + suburb\n",
    "\n",
    "            #if url not in base_urls:\n",
    "            base_urls.append(url)\n",
    "            #else:\n",
    "            #    print(\"Duplicate\")\n",
    "    else:\n",
    "        print(\"Collecing for whole state of NSW\")\n",
    "        base_urls.append('https://www.stayz.com.au/accommodation/nsw')\n",
    "\n",
    "\n",
    "    # Find why the others were not identified?? 4k missing??\n",
    "    return base_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting by individual area...\n",
      "Collecting by individual area...\n",
      "978\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the list of base urls\n",
    "base_urls_df = pd.DataFrame(data=get_base_urls(True), columns=['url_area'])\n",
    "nsw_urls_df = pd.DataFrame(data=get_nsw_urls(True), columns=['url_nsw'])\n",
    "\n",
    "\n",
    "\n",
    "comb_data = nsw_urls_df.merge(base_urls_df, how='left',left_on=['url_nsw'], right_on=['url_area'])\n",
    "\n",
    "comb_data.head(10)\n",
    "\n",
    "comb_data.fillna('')\n",
    "\n",
    "print(str(len(base_urls_df)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
